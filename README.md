# attention-large-vrp
The code documents accompanying the paper 'An Attention Based Solver, a Clustering Algorithm, and a Very Large Scale Instance of the Vehicle Routing Problem Walk Into a Bar...'. This paper aims to test whether solvers based on attention mechanisms are able to efficiently solve very large scale instances of the Vehicle Routing Problem (VRP). 

To do this three solvers are applied based on the work of Kool, van Hoof, and Welling's 2019 paper, ‘Attention, learn to solve routing problems!’, Xin, Song, Cao, and Zhang's 2020 paper ‘Step-wise deep learning models for solving routing problems’, and Xin, Song, Cao, and Zhang's 2021 paper ‘Multi-decoder attention model with embedding glimpse for solving vehicle routing problems’. The code from these paper's are available at https://github.com/wouterkool/attention-learn-to-route, https://github.com/liangxinedu/stepwise, and https://github.com/liangxinedu/MDAM respectively. Throughout the paper (and in this Git repository), the models are refferred to as Kool, ASW-TAM, and MDAM respectively. The code for these solvers has been slightly altered for the use cases of this assignment and are therefore included in the repository.

The very large scale instances of the VRP are the benchmark datasets laid out in Arnold, Gendreau, and So ̈rensen's 2019 paper ‘Efficiently solving very large-scale routing problems’. These datasets consist of ten instances that contain between 3000 and 30000 customer nodes. They are based on parcel distributions in Belgium. The datasets were obtained from http://vrp.galgos.inf.puc-rio.br/index.php/en/, and are included in this repository.

These solvers are traditionally used to solve smaller instances of the VRP (100 nodes), where node locations are uniformly generated between 0 and 1, and where hubs are centrally located. This forms the intuition of the paper, whereby larger datasets can be clustered and solved individually. However, this presents two key problems. The first is that the clustering algorithm used (K-means) does not produce uniform size clusters. This means that instead of being able to feed in a stack of clusters, each has to be solved individually, which means this method is unable to capture the processing power of parallel computing. The second problem is that because the solvers used in this paper are trained using reinforcement learning, they become good at solving the specific type of VRP they are used to seeing. These are the VRPs that are uniformly generated between 0 and 1, and have fixed size nodes (20, 50, and 100).

The first problem is not addressed in this paper. The second problem is addressed by having several data setups and using solvers trained on various instances of the VRP to see which performs the best. There are four data setups known as test 1, test 2, test 3, and test 4 throughout the paper. Test 1 is the cluster with the actual depot included. Test 2 is the cluster with the depot excluded and the centroid acts as an intermediary depot. Test 3 is Test 1 however, all locations have been normalised using a min max scalar. Test 4 is test 2, where all locations have been normalised using a min max scalar. Results are produce by applying the three solvers trained on 20, 50, and 100 node instances to each of the tests.

## Data Set Up 
The Data Set Up file contains the Jupyter file that is responsible for reading in the data from http://vrp.galgos.inf.puc-rio.br/index.php/en/ and for setting up the datasets (test 1, test 2, test 3, and test 4) that will be used in the solvers. This involves clustering and normalizing. Furthermore this file contains the code that generates the findings for the preliminary data analysis. All the generated datasets are written to a Google Drive Account.

## Applying the Solvers
Each solver contains three trained models based on 20, 50, and 100 node instances of the VRP. Furthermore, the instructions to run each model are avaialbe in the Git repositories (https://github.com/wouterkool/attention-learn-to-route, https://github.com/liangxinedu/stepwise, and https://github.com/liangxinedu/MDAM). Using these instructions, Jupyter files were created for each of the solvers. Each of these Jupyter files contains a function that generates a bash script, which is then subsequently run. The reason for this is because each cluster from a particular problem has to be fed in indiviudally into the solver. For instance, if there are 100 clusters, all 100 need to be solved individually. As such, the script calls the base path using the instance name (Antwerp1 for example), variations (which are the four tests), models (which are the 3 trained models), and the framework used (Kool, ASW-TAM, or MDAM). In turn this feeds the desired dataset into the solver, using the desired pretrained weights and biases, and generates a solution. The solvers are modified to wirte the solutions to a desired directory. The use of bash scripts is essential to this paper as there are many variations of each solver to be run with many datasets. 

## Analysing Results
Because the depot is not included in test 3 and test 4, the result analysis accounts for this fact and adds in the neccesary distance calculations. Furthermore, this file is responsible for creating the graphics used in the paper.






